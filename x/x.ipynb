{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire Kaggle Files\n",
    "filename = 'c:/pie/make_money_with_machine_learning/3-fraud-prediction/kaggle.json'\n",
    "file = open(filename)\n",
    "userdetails = json.load(file)\n",
    "print(userdetails)\n",
    "username = userdetails['username']\n",
    "apikey = userdetails['key']\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = username \n",
    "os.environ['KAGGLE_KEY'] = apikey\n",
    "!kaggle competitions download -c ieee-fraud-detection\n",
    "\n",
    "zf = ZipFile('train_transaction.csv.zip', 'r')\n",
    "zf.extractall()\n",
    "zf.close()\n",
    "zf = ZipFile('train_identity.csv.zip', 'r')\n",
    "zf.extractall()\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Merge Dataframes\n",
    "train_id = pd.read_csv('train_identity.csv', low_memory=False)\n",
    "train_transaction = pd.read_csv('train_transaction.csv', low_memory=False)\n",
    "df_train_transaction = pd.read_csv(train_transaction, low_memory=False)\n",
    "df_train_identity = pd.read_csv(train_id, low_memory=False)\n",
    "df_train = pd.merge(df_train_transaction, df_train_identity, on='TransactionID', how='left')\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things I decided to do, was check how imbalanced this dataset is, considering that Fraud Detection datasets are known to have a high amount of imbalance between isFraud=0 and isFraud=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isfraud = df_train[df_train.isFraud==1]\n",
    "df_notfraud = df_train[df_train.isFraud==0]\n",
    "imbalance_percentage = len(df_isfraud)/len(df_notfraud)\n",
    "\n",
    "print('df_train: '+ str(df_train.shape))\n",
    "print('df_isfraud: '+ str(df_isfraud.shape))\n",
    "print('df_notfraud: '+ str(df_notfraud.shape))\n",
    "print('class imbalance percentage: ' + str(round(imbalance_percentage, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Only 3% of the rows of this dataframe are identified as isFraud=1, the other 97% are isFraud=0. Very imbalanced! Be careful to acknowledge that predictions can be dramatically biased based on this fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection based on the percentage of NA's in a columns\n",
    "df_na = pd.DataFrame({\"na_count\":df_train.isnull().sum().sort_values(ascending=False)})\n",
    "df_na[\"percentage\"]=df_na[\"na_count\"]/len(df_train)\n",
    "limit=0.15 \n",
    "selected_features = df_na[df_na[\"percentage\"]<limit].index \n",
    "df = df_train[df_train.columns.intersection(selected_features)]\n",
    "\n",
    "# Handling our Categorical Variables\n",
    "# Determine which categorical variables should remain, and what they are.\n",
    "list_categorical_columns = []\n",
    "list_categorical_columns_remaining = df[df.select_dtypes(include=['object']).columns].columns.tolist()\n",
    "print('Remaining Categorical Columns: ' + ', '.join(list_categorical_columns_remaining))\n",
    "\n",
    "# Create dummy variable dataframes to encode categorical variables into numbers\n",
    "for categorical_column_name in list_categorical_columns_remaining:\n",
    "    list_categorical_columns.append(categorical_column_name)\n",
    "\n",
    "# Concatenate our dummy dataframes into our primary dataframe, \n",
    "# then get rid of the non-encoded variables\n",
    "df_dummies = pd.get_dummies(df[list_categorical_columns])\n",
    "df = pd.concat([df, df_dummies], axis=1)\n",
    "df = df._get_numeric_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "series_nanvalues = pd.isnull(df_train).sum().sort_values(ascending=False)\n",
    "percentage_nanvalues = (miss_data/len(df_train))*100\n",
    "df_na = pd.concat(objs = [series_nanvalues, percentage_nanvalues], keys=['Columns','Pct_Nan'], axis=1)\n",
    "df_na_90 = df_na[df_na.Pct_Nan >= 90]\n",
    "df_na_80 = df_na[df_na.Pct_Nan >= 80]\n",
    "percentage_nans_under_80 = str(round(len(df_na_90)/len(df_na),4))\n",
    "\n",
    "print('Out of all of the original 434 columns, only ' + percentage_nans_under_80 + ' columns are composed of fewer than 80% NaN Values.')\n",
    "print('Below, we can see a Bar Plot showcasing the percentage of variables which are the most-highly populated by NaN values.')\n",
    "sns.set(style=\"darkgrid\", font_scale=5)\n",
    "plt.figure(figsize=(100,25))\n",
    "p = sns.barplot(x = 'Columns', y='Pct_Nan', data=df_na_90)\n",
    "p.set_xticklabels(list(df_train.columns))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training our model\n",
    "df2 = df.copy()\n",
    "df2.fillna(value=df2.median(), inplace=True)\n",
    "X = np.array(df2.drop(['isFraud'], axis=1))\n",
    "X = preprocessing.scale(X)\n",
    "y = np.array(df2['isFraud'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "classifier_logistic = LogisticRegression(solver='lbfgs')\n",
    "classifier_logistic.fit(X_train, y_train)\n",
    "confidence_logistic = round(classifier_logistic.score(X_test, y_test),6)\n",
    "print('\\tLogistic Confidence: ' + str(confidence_logistic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
