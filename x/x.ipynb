{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Dependencies\nimport os\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom zipfile import ZipFile\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom google.colab import drive\ndrive.mount('/content/drive')","execution_count":102,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-102-038756df8b9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Acquire Kaggle Files\nfilename = 'kaggle.json'\nfile = open(filename)\nuserdetails = json.load(file)\nusername = userdetails['username']\napikey = userdetails['key']\n\nos.environ['KAGGLE_USERNAME'] = username \nos.environ['KAGGLE_KEY'] = apikey\n!kaggle competitions download -c ieee-fraud-detection\n\nzf = ZipFile('train_transaction.csv.zip', 'r')\nzf.extractall()\nzf.close()\nzf = ZipFile('train_identity.csv.zip', 'r')\nzf.extractall()\nzf.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Merge Dataframes\ntrain_id = pd.read_csv('train_identity.csv', low_memory=False)\ntrain_transaction = pd.read_csv('train_transaction.csv', low_memory=False)\ndf_train_transaction = pd.read_csv(train_transaction, low_memory=False)\ndf_train_identity = pd.read_csv(train_id, low_memory=False)\ndf_train = pd.merge(df_train_transaction, df_train_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the first things I decided to do, was check how imbalanced this dataset is, considering that Fraud Detection datasets are known to have a high amount of imbalance between isFraud=0 and isFraud=1."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_isfraud = df_train[df_train.isFraud==1]\ndf_notfraud = df_train[df_train.isFraud==0]\nimbalance_percentage = len(df_isfraud)/len(df_notfraud)\n\nprint('df_train: '+ str(df_train.shape))\nprint('df_isfraud: '+ str(df_isfraud.shape))\nprint('df_notfraud: '+ str(df_notfraud.shape))\nprint('class imbalance percentage: ' + str(round(imbalance_percentage, 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! Only 3% of the rows of this dataframe are identified as isFraud=1, the other 97% are isFraud=0. Very imbalanced! Be careful to acknowledge that predictions can be dramatically biased based on this fact."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Selection based on the percentage of NA's in a columns\ndf_na = pd.DataFrame({\"na_count\":df_train.isnull().sum().sort_values(ascending=False)})\ndf_na[\"percentage\"]=df_na[\"na_count\"]/len(df_train)\nlimit=0.15 \nselected_features = df_na[df_na[\"percentage\"]<limit].index \ndf = df_train[df_train.columns.intersection(selected_features)]\n\n# Handling our Categorical Variables\n# Determine which categorical variables should remain, and what they are.\nlist_categorical_columns = []\nlist_categorical_columns_remaining = df[df.select_dtypes(include=['object']).columns].columns.tolist()\nprint('Remaining Categorical Columns: ' + ', '.join(list_categorical_columns_remaining))\n\n# Create dummy variable dataframes to encode categorical variables into numbers\nfor categorical_column_name in list_categorical_columns_remaining:\n    list_categorical_columns.append(categorical_column_name)\n\n# Concatenate our dummy dataframes into our primary dataframe, \n# then get rid of the non-encoded variables\ndf_dummies = pd.get_dummies(df[list_categorical_columns])\ndf = pd.concat([df, df_dummies], axis=1)\ndf = df._get_numeric_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nseries_nanvalues = pd.isnull(df_train).sum().sort_values(ascending=False)\npercentage_nanvalues = (miss_data/len(df_train))*100\ndf_na = pd.concat(objs = [series_nanvalues, percentage_nanvalues], keys=['Columns','Pct_Nan'], axis=1)\ndf_na_90 = df_na[df_na.Pct_Nan >= 90]\ndf_na_80 = df_na[df_na.Pct_Nan >= 80]\npercentage_nans_under_80 = str(round(len(df_na_90)/len(df_na),4))\n\nprint('Out of all of the original 434 columns, only ' + percentage_nans_under_80 + ' columns are composed of fewer than 80% NaN Values.')\nprint('Below, we can see a Bar Plot showcasing the percentage of variables which are the most-highly populated by NaN values.')\nsns.set(style=\"darkgrid\", font_scale=5)\nplt.figure(figsize=(100,25))\np = sns.barplot(x = 'Columns', y='Pct_Nan', data=df_na_90)\np.set_xticklabels(list(df_train.columns))\np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training our model\ndf2 = df.copy()\ndf2.fillna(value=df2.median(), inplace=True)\nX = np.array(df2.drop(['isFraud'], axis=1))\nX = preprocessing.scale(X)\ny = np.array(df2['isFraud'])\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0)\n\nclassifier_logistic = LogisticRegression(solver='lbfgs')\nclassifier_logistic.fit(X_train, y_train)\nconfidence_logistic = round(classifier_logistic.score(X_test, y_test),6)\nprint('\\tLogistic Confidence: ' + str(confidence_logistic))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}