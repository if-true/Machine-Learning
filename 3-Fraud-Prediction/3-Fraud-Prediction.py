# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cjgg7WLUCSO4p3JPikY9KlS0hxUkyYbg
"""

import os
import numpy as np
import pandas as pd
from glob import glob
import seaborn as sns
import matplotlib as mpl
from zipfile import ZipFile
from matplotlib import style
import matplotlib.pyplot as plt
from google.colab import drive
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

mpl.rc('figure', figsize=(8, 7))
style.use('ggplot')
drive.mount('/content/drive')

from google.colab import files
files.upload()  #this will prompt you to upload the kaggle.json

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json  # set permission

!kaggle competitions download -c ieee-fraud-detection

zf = ZipFile('train_transaction.csv.zip', 'r')
zf.extractall()
zf.close()
zf = ZipFile('train_identity.csv.zip', 'r')
zf.extractall()
zf.close()

# Open DataFraes, and Merge
df_train_transaction = pd.read_csv('train_transaction.csv', low_memory=False)
df_train_identity = pd.read_csv('train_identity.csv', low_memory=False)
df_train = pd.merge(df_train_transaction, df_train_identity, 
  on='TransactionID',
how='left')
df_train.shape

"""**One of the first things I decided to do, was check how imbalanced this dataset is, considering that Fraud Detection datasets are known to have a high amount of imbalance between isFraud=0 and isFraud=1.**"""

df_isfraud = df_train[df_train.isFraud==1]
imbalance_percentage = len(df_isfraud)/len(df_train)

print('df_train: '+ str(df_train.shape))
print('df_isfraud: '+ str(df_isfraud.shape))
print('class imbalance percentage: ' + str(round(imbalance_percentage, 5)))

"""**Wow! Only 3% of the rows of this dataframe are identified as isFraud=1, the other 97% are isFraud=0. Very imbalanced! Be careful to acknowledge that predictions can be dramatically biased based on this fact.**"""

# Feature Selection based on the percentage of NA's in a columns
df_na = pd.DataFrame({"na_count":df_train.isnull().sum().sort_values(ascending=False)})
df_na["percentage"]=df_na["na_count"]/len(df_train)
limit=0.15 
selected_features = df_na[df_na["percentage"]<limit].index 
df = df_train[df_train.columns.intersection(selected_features)]

# Handling our Categorical Variables
# Determine which categorical variables should remain, and what they are.
list_categorical_columns = []
list_categorical_columns_remaining = df[df.select_dtypes(include=['object']).columns].columns.tolist()
print('Remaining Categorical Columns: ' + ', '.join(list_categorical_columns_remaining))

# Create dummy variable dataframes to encode categorical variables into numbers
for categorical_column_name in list_categorical_columns_remaining:
    list_categorical_columns.append(categorical_column_name)

# Concatenate our dummy dataframes into our primary dataframe, 
# then get rid of the non-encoded variables
df_dummies = pd.get_dummies(df[list_categorical_columns])
df = pd.concat([df, df_dummies], axis=1)
df = df._get_numeric_data()

series_nanvalues = pd.isnull(df_train).sum().sort_values(ascending=False)
percentage_nanvalues = (series_nanvalues/len(df_train))*100
df_na = pd.concat(objs = [series_nanvalues, percentage_nanvalues], keys=['Columns','Pct_Nan'], axis=1)
df_na_90 = df_na[df_na.Pct_Nan >= 90]
df_na_80 = df_na[df_na.Pct_Nan >= 80]
percentage_nans_under_80 = str(round(len(df_na_90)/len(df_na),4))

print('Out of all of the original 434 columns, only ' + percentage_nans_under_80 + ' columns are composed of fewer than 80% NaN Values.')
print('Below, we can see a Bar Plot showcasing the percentage of variables which are the most-highly populated by NaN values.')
sns.set(style="darkgrid", font_scale=1)
plt.figure(figsize=(20,5))
p = sns.barplot(x = 'Columns', y='Pct_Nan', data=df_na_90)
p.set_xticklabels(list(df_train.columns))
p

# training our model
df2 = df.copy()
df2.fillna(value=df2.median(), inplace=True)
X = np.array(df2.drop(['isFraud'], axis=1))
X = preprocessing.scale(X)
y = np.array(df2['isFraud'])
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0)

classifier_logistic = LogisticRegression(solver='lbfgs')
classifier_logistic.fit(X_train, y_train)
confidence_logistic = round(classifier_logistic.score(X_test, y_test),6)
print('\tLogistic Confidence: ' + str(confidence_logistic) + '\n')